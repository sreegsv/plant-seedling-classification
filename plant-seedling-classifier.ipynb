{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7880,"databundleVersionId":862031,"sourceType":"competition"},{"sourceId":42119951,"sourceType":"kernelVersion"}],"dockerImageVersionId":30004,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! ls '/kaggle/input/plant-seedlings-classification/'\n! ls '/kaggle/input/oversampling-plant-seedling/'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport PIL\nimport PIL.Image\nimport tensorflow as tf\nprint(tf.__version__)\nprint(\"Is there a GPU available: \", end = ' '),\nprint(tf.test.is_gpu_available())\nimport cv2\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nmemory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting all the variables and constants as configuration\nconfiguration = {\n    'train_path_org': '/kaggle/input/plant-seedlings-classification/train',\n    'train_path': '/kaggle/input/oversampling-plant-seedling/balanced_data',\n    'test_path': '/kaggle/input/plant-seedlings-classification/test/',\n    'csv_path': '/kaggle/input/plant-seedlings-classification/sample_submission.csv',\n    \n    'validation_split': 0.25,\n    'buffer_size': 1000,\n    'batch_size': 32,\n    'img_height': 250,\n    'img_width': 250,\n    'num_classes' : 12,\n    \n    'drop_out_rate': 0.2,\n    'epochs': 20,\n    'fine_tune_flag': True,\n    'fine_tune_epochs': 20,\n    'vgg_finetune': 17,\n    'resnet_finetune': 171,\n    'inception_finetune': 251,\n    \n    'early_stopping': tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                    mode='auto',\n                                    verbose=1,\n                                    patience=5),\n    'reduce_lr_on_platue': tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                    patience=3,\n                                    verbose=1,\n                                    factor=.5, \n                                    min_lr=0.0000001),\n    \n    'vgg_checkpoint': tf.keras.callbacks.ModelCheckpoint('vgg_model.h5', \n                                    monitor='val_loss', \n                                    verbose=1, \n                                    save_best_only=True, \n                                    mode='min'),\n    'resnet_checkpoint': tf.keras.callbacks.ModelCheckpoint('resnet_model.h5', \n                                    monitor='val_loss', \n                                    verbose=1, \n                                    save_best_only=True, \n                                    mode='min'),\n    'inception_checkpoint': tf.keras.callbacks.ModelCheckpoint('inception_model.h5', \n                                    monitor='val_loss', \n                                    verbose=1, \n                                    save_best_only=True, \n                                    mode='min'),\n    \n    'optimizer': tf.keras.optimizers.Adam(),\n    'loss': tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n    'accuracy': ['accuracy'],\n    \n    'vgg_16_base': tf.keras.applications.VGG16(\n                                input_shape=(250, 250, 3),\n                                include_top=False,\n                                weights='imagenet'),\n    'resnet_50_base': tf.keras.applications.ResNet50(\n                                input_shape=(250, 250, 3),\n                                include_top=False,\n                                weights='imagenet'),\n    'inception_v3_base': tf.keras.applications.InceptionV3(\n                                input_shape=(250, 250, 3),\n                                include_top=False,\n                                weights='imagenet'), \n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Class Balance of original dataset","metadata":{}},{"cell_type":"code","source":"path = configuration['train_path_org']\nfolder_name, img_count = [], []\nfor i in os.listdir(path):\n    folder_name.append(i)\n    count = len(os.listdir(os.path.join(path,i)))\n    img_count.append(count)\n    print(\"Folder Name : \",i)\n    print(\"No of Images : \",count)\n\nfig = plt.figure(figsize = (20, 5)) \n  \n# creating the bar plot \nplt.bar(folder_name, img_count, color ='maroon',  \n        width = 0.4) \n  \nplt.xlabel(\"Folder Name\") \nplt.ylabel(\"No of images in each folder\") \nplt.title(\"Image Frequency\")\n\nfig.autofmt_xdate()\nplt.show() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Class Balance of oversampled dataset","metadata":{}},{"cell_type":"code","source":"path = configuration['train_path']\nfolder_name, img_count = [], []\nfor i in os.listdir(path):\n    folder_name.append(i)\n    count = len(os.listdir(os.path.join(path,i)))\n    img_count.append(count)\n    print(\"Folder Name : \",i)\n    print(\"No of Images : \",count)\n\nfig = plt.figure(figsize = (20, 5)) \n  \n# creating the bar plot \nplt.bar(folder_name, img_count, color ='maroon',  \n        width = 0.4) \n  \nplt.xlabel(\"Folder Name\") \nplt.ylabel(\"No of images in each folder\") \nplt.title(\"Image Frequency\")\n\nfig.autofmt_xdate()\nplt.show() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Working with tf.data","metadata":{}},{"cell_type":"code","source":"import pathlib\ndata_dir = pathlib.Path(configuration['train_path'])\nprint(data_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_count = len(list(data_dir.glob('*/*.png')))\nprint(image_count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\nlist_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)\n\n# For Inception\n# as it requires different type of preprocessing compared to vgg and resnet\ninception_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\ninception_ds = inception_ds.shuffle(image_count, reshuffle_each_iteration=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for f in list_ds.take(3):\n    print(f.numpy())\n    s = f.numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\nprint(class_names)\nclass_map = {}\nfor i in range(len(class_names)):\n    class_map[i] = class_names[i]\nprint('\\n\\n',class_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_size = int(image_count * 0.2)\n\ntrain_ds = list_ds.skip(val_size)\nval_ds = list_ds.take(val_size)\n\ninception_train = inception_ds.skip(val_size)\ninception_val = inception_ds.take(val_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tf.data.experimental.cardinality(train_ds).numpy())\nprint(tf.data.experimental.cardinality(val_ds).numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_label(file_path):\n    # convert the path to a list of path components\n    parts = tf.strings.split(file_path, os.path.sep)\n    # The second to last is the class-directory\n    one_hot = parts[-2] == class_names\n    label_classes = list(range(12))\n    tensor = tf.constant(label_classes,dtype = tf.int64)\n    ans = tf.boolean_mask(tensor,one_hot)\n    return ans[0]\n\ndef augment(image):\n    image = tf.image.random_brightness(image, max_delta=0.2) # Random brightness\n    image = tf.image.random_contrast(image, 0.8, 1)\n    return image\n\ndef decode_img(img):\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_jpeg(img, channels=3)\n    \n    # Preprocess\n    img = tf.keras.applications.vgg16.preprocess_input(img)\n    \n    # Cast\n    img = tf.cast(img,tf.float32)\n    \n    # Augment\n    img = augment(img)\n    \n    # resize the image to the desired size\n    img = tf.image.resize(img, [configuration['img_height'], configuration['img_width']])\n    return img\n\ndef process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label\n\nlabeled_ds = list_ds.map(process_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\nval_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\ndef configure_for_performance(ds):\n    ds = ds.cache()\n    ds = ds.shuffle(buffer_size=configuration['buffer_size'])\n    ds = ds.batch(configuration['batch_size'])\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\ntrain_ds = configure_for_performance(train_ds)\nval_ds = configure_for_performance(val_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inception_img(img):\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_jpeg(img, channels=3)\n    \n    # Cast\n    img = tf.cast(img,tf.float32)\n    img = img/255.\n    \n    # Augment\n    img = augment(img)\n    \n    # resize the image to the desired size\n    img = tf.image.resize(img, [configuration['img_height'], configuration['img_width']])\n    return img\n\n\ndef process_inception(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = inception_img(img)\n    return img, label\n\ninception_ds = list_ds.map(process_inception)\n\nfor image, label in inception_ds.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())\n    plt.imshow(image)\n    plt.title(class_map[label.numpy()])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_batch, label_batch =  [],[]\n\nfor image, label in inception_ds.take(10):\n    image_batch.append(image)\n    label_batch.append(label)\n\n\nplt.figure(figsize=(10, 10))\nfor i in range(6):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i].numpy().astype(\"float32\"))\n    label = label_batch[i]\n    plt.title(class_names[label])\n    plt.axis(\"off\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inception_train = inception_train.map(process_inception, num_parallel_calls=AUTOTUNE)\ninception_val = inception_val.map(process_inception, num_parallel_calls=AUTOTUNE)\n\ninception_train = configure_for_performance(inception_train)\ninception_val = configure_for_performance(inception_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model template","metadata":{}},{"cell_type":"code","source":"def build_model(base_model):\n    \n    base_model.trainable = False\n    \n    model = tf.keras.Sequential([\n        base_model,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(configuration['num_classes']),\n    ])\n\n    model.compile(\n        optimizer  = configuration['optimizer'],\n        loss = configuration['loss'],\n        metrics = configuration['accuracy']\n    )\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Accuracy and Loss plotting template","metadata":{}},{"cell_type":"code","source":"def plot_graphs(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(plt.ylim()),1])\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Cross Entropy')\n    plt.ylim([0,1.0])\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VGG 16","metadata":{}},{"cell_type":"code","source":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg_base = configuration['vgg_16_base']\nvgg_model = build_model(vgg_base)\n\nprint(vgg_model.summary())\nprint('\\n\\n')\n\nhistory = vgg_model.fit(\n    train_ds,\n    validation_data = val_ds,\n    epochs = configuration['epochs'],\n    callbacks = [\n                configuration['early_stopping'], \n                configuration['reduce_lr_on_platue'], \n                configuration['vgg_checkpoint']\n                ]\n    )\n\n# Plotting the graphs\nplot_graphs(history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tune VGG 16","metadata":{}},{"cell_type":"code","source":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg_base.trainable = True\nprint('Total No of layers: ',len(vgg_base.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = configuration['vgg_finetune'] # 17\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in vgg_base.layers[:fine_tune_at]:\n    layer.trainable =  False\n    \n# Check the trainable status of the individual layers\nfor layer in vgg_base.layers:\n    print(layer, layer.trainable)\n\nprint('\\n\\n')\n    \nvgg_model.compile(\n    optimizer  = configuration['optimizer'],\n    loss = configuration['loss'],\n    metrics = configuration['accuracy']\n)\n\nprint('No of trainable variables: ',len(vgg_model.trainable_variables))\nprint(vgg_model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if(configuration['fine_tune_flag']):\n    history_fine = vgg_model.fit(train_ds,\n                             validation_data=val_ds,\n                             epochs=configuration['fine_tune_epochs'],\n                             callbacks=[configuration['early_stopping'], \n                                        configuration['reduce_lr_on_platue'], \n                                        configuration['vgg_checkpoint']]\n                            )\n    \n    # Plotting the graphs\n    plot_graphs(history_fine)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VGG16 Classification Report","metadata":{}},{"cell_type":"code","source":"loaded_model = tf.keras.models.load_model('vgg_model.h5')\ny_true = []\ny_pred = []\n\nfor image,label in val_ds.take(-1):\n    ans = loaded_model.predict(image)\n    for i in range(32):\n        ans_predicted = np.argmax(ans[i])\n        ans_actual = label[i].numpy()\n        y_true.append(ans_actual)\n        y_pred.append(ans_predicted)\n\nprint(class_map)\nprint('\\n\\nClassification Report: ')\nprint(classification_report(y_true, y_pred, target_names=class_map.values()))\nprint('\\n\\nConfusion Matrix: ')\nprint(confusion_matrix(y_true, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sn\n%matplotlib inline\n\ndata = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (20,6))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='d')# font size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resnet 50","metadata":{}},{"cell_type":"code","source":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_base = configuration['resnet_50_base']\nresnet_model = build_model(resnet_base)\n\nprint(resnet_model.summary())\nprint('\\n\\n')\n\nhistory = resnet_model.fit(\n    train_ds,\n    validation_data = val_ds,\n    epochs = configuration['epochs'],\n    callbacks = [\n                configuration['early_stopping'], \n                configuration['reduce_lr_on_platue'], \n                configuration['resnet_checkpoint']\n                ]\n    )\n\n# Plotting the graphs\nplot_graphs(history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tune ResNet 50","metadata":{}},{"cell_type":"code","source":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_base.trainable = True\nprint('Total No of layers: ',len(resnet_base.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = configuration['resnet_finetune'] # 171\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in resnet_base.layers[:fine_tune_at]:\n    layer.trainable =  False\n\nprint('\\n\\n')\n    \nresnet_model.compile(\n    optimizer  = configuration['optimizer'],\n    loss = configuration['loss'],\n    metrics = configuration['accuracy']\n)\n\nprint('No of trainable variables: ',len(resnet_model.trainable_variables))\nprint(resnet_model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if(configuration['fine_tune_flag']):\n    history_fine = resnet_model.fit(train_ds,\n                             validation_data=val_ds,\n                             epochs=configuration['fine_tune_epochs'],\n                             callbacks=[configuration['early_stopping'],\n                                        configuration['reduce_lr_on_platue'], \n                                        configuration['resnet_checkpoint']]\n                            )\n    \n    # Plotting the graphs\n    plot_graphs(history_fine)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ResNet Classification Report","metadata":{}},{"cell_type":"code","source":"loaded_model = tf.keras.models.load_model('resnet_model.h5')\ny_true = []\ny_pred = []\n\nfor image,label in val_ds.take(-1):\n    ans = loaded_model.predict(image)\n    for i in range(32):\n        ans_predicted = np.argmax(ans[i])\n        ans_actual = label[i].numpy()\n        y_true.append(ans_actual)\n        y_pred.append(ans_predicted)\n\nprint(class_map)\nprint('\\n\\nClassification Report: ')\nprint(classification_report(y_true, y_pred, target_names=class_map.values()))\nprint('\\n\\nConfusion Matrix: ')\nprint(confusion_matrix(y_true, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sn\n%matplotlib inline\n\ndata = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (20,6))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='d')# font size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inception V3","metadata":{}},{"cell_type":"code","source":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inception_base = configuration['inception_v3_base']\ninception_model = build_model(inception_base)\n\nprint(inception_model.summary())\nprint('\\n\\n')\n\nhistory = inception_model.fit(\n    inception_train,\n    validation_data = inception_val,\n    epochs = configuration['epochs'],\n    callbacks = [\n                configuration['early_stopping'], \n                configuration['reduce_lr_on_platue'], \n                configuration['inception_checkpoint']\n                ]\n    )\n\n# Plotting the graphs\nplot_graphs(history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tune InceptionNet v3","metadata":{}},{"cell_type":"code","source":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inception_base.trainable = True\nprint('Total No of layers: ',len(inception_base.layers))\n\n# Fine-tune from this layer onwards\n# fine_tune_at = configuration['inception_finetune']\nfine_tune_at = 251\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in inception_base.layers[:fine_tune_at]:\n    layer.trainable =  False\n    \ninception_model.compile(\n    optimizer  = configuration['optimizer'],\n    loss = configuration['loss'],\n    metrics = configuration['accuracy']\n)\n\nprint('No of trainable variables: ',len(inception_model.trainable_variables))\nprint(inception_model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if(configuration['fine_tune_flag']):\n    history_fine = inception_model.fit(inception_train,\n                             validation_data=inception_val,\n                             epochs=configuration['fine_tune_epochs'],\n                             callbacks=[configuration['early_stopping'], \n                                        configuration['reduce_lr_on_platue'], \n                                        configuration['inception_checkpoint']]\n                            )\n    \n    # Plotting the graphs\n    plot_graphs(history_fine)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inception Classification Report","metadata":{}},{"cell_type":"code","source":"loaded_model = tf.keras.models.load_model('inception_model.h5')\ny_true = []\ny_pred = []\n\nfor image,label in inception_val.take(-1):\n    ans = loaded_model.predict(image)\n    for i in range(32):\n        ans_predicted = np.argmax(ans[i])\n        ans_actual = label[i].numpy()\n        y_true.append(ans_actual)\n        y_pred.append(ans_predicted)\n\nprint(class_map)\nprint('\\n\\nClassification Report: ')\nprint(classification_report(y_true, y_pred, target_names=class_map.values()))\nprint('\\n\\nConfusion Matrix: ')\nprint(confusion_matrix(y_true, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sn\n%matplotlib inline\n\ndata = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (20,6))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='d')# font size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"memory_freed = gc.collect()\nprint('Memory made free: ',memory_freed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
